# -*- coding: utf-8 -*-
"""MaxTemp_KAN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wgPJ24W6XDuUivg0R_t_nuUTKbifI_vw
"""

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt

# Load file
data = pd.read_excel("clima2.xlsx")

# Show first rows
print(data.head())

"""## Preprocessing"""

# Create an object LabelEncoder to code the 'Edo' column
label_encoder = LabelEncoder()

# Code the 'Edo' column and add it to the dataset
data['Edo_encoded'] = label_encoder.fit_transform(data['Edo'])

# Check and delete null values
data = data.dropna(subset=['Mes', 'Tmax', 'Lon', 'Lat', 'Edo', 'Año'])

# Select relevant features and target
features = ['Lon', 'Lat', 'Edo_encoded', 'Mes', 'Año']
target = 'Tmax'

# Divide dataset into training, validation and test sets
train_data = data[data['Año'] <= 2022]
val_data = data[data['Año'] == 2023]
test_data = data[data['Año'] == 2024]

# Separate features and target for each dataset
X_train, y_train = train_data[features], train_data[target]
X_val, y_val = val_data[features], val_data[target]
X_test, y_test = test_data[features], test_data[target]

"""## Data exploration and visualization"""

# Statistical summary of the features
print("Statistical summary of the features:")
print(data.describe())

# Unique states in the dataset
print("Unique states in the dataset:")
print(data['Edo'].unique())

# Max Temp Distribution
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10, 6))
sns.histplot(data['Tmax'], bins=30, kde=True)
plt.title('Max Temp Distribution')
plt.xlabel('Tmax')
plt.ylabel('Frequence')
plt.show()

# Max Temp evolution over the years
plt.figure(figsize=(12, 6))
sns.lineplot(x='Año', y='Tmax', data=data, errorbar=None)
plt.title('Max Temp evolution over the years')
plt.xlabel('Año')
plt.ylabel('Tmax')
plt.show()

# Feature Correlation
plt.figure(figsize=(10, 8))
sns.heatmap(data[['Lon', 'Lat', 'Mes', 'Año', 'Tmax']].corr(), annot=True, cmap='coolwarm')
plt.title('Correlation matrix')
plt.show()

# Geographic distribution of Max Temp
plt.figure(figsize=(10, 6))
plt.scatter(data['Lon'], data['Lat'], c=data['Tmax'], cmap='viridis', alpha=0.7)
plt.colorbar(label='Tmax')
plt.title('Geographic distribution of Max Temp')
plt.xlabel('Long')
plt.ylabel('Lat')
plt.show()

"""# KAN Implementation

## Building the model
"""

def build_kan(input_shape):
    model = tf.keras.Sequential([
        layers.Input(shape=input_shape),
        layers.Dense(128, activation='relu'),
        layers.Dense(64, activation='relu'),
        layers.Dense(1)  # Output layer to predict Tmax
    ])
    model.compile(optimizer='adam', loss='mse', metrics=['mae'])
    return model

# Model build
model = build_kan(X_train.shape[1:])

"""## Training the model

"""

# Model training
history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=50,
    batch_size=32
)

"""## Model evaluation and 2028 predictions"""

# Model evaluation in test set
test_loss, test_mae = model.evaluate(X_test, y_test)
print(f"Test MAE: {test_mae}")

# Print unique values from Edo_encoded and their corresponding states
print(data[['Edo', 'Edo_encoded']].drop_duplicates().sort_values(by='Edo_encoded'))

# 2028 predictions

future_data = pd.DataFrame({
    'Edo_encoded': [24] * 6,  # Edo_encoded repetido n veces (una para cada mes)
    'Mes': list(range(1, 7)),  # Meses del 1 al n+1
    'Año': [2028] * 6,        # El año 2028 para todas las predicciones
    'Lon': [-100.38] * 6,    # Longitudes para cada estado
    'Lat': [20.58] * 6       # Latitudes correspondientes
})

# NOTE = Input columns should be in the same order as when the model was trained
future_data = future_data[['Lon', 'Lat', 'Edo_encoded', 'Mes', 'Año']]

# Predictions
predictions = model.predict(future_data)
print(predictions)